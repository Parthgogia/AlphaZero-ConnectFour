{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75fe9baf-f13a-4ad4-862c-9ef383c01cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "2.8.0+cu129\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "import math\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "from tqdm.auto import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d684cd24-e2c7-42d4-aa63-75ab1efe0ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\parth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\parth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675b92e7-cdee-4277-b4ed-b8f50c80bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\anaconda3\\python.exe\n",
      "C:\\Users\\Parth\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n",
      "C:\\Users\\Parth\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n",
      "C:\\msys64\\ucrt64\\bin\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa0854a-d3e1-4c19-ba6b-649d0348de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2605ace0-91a5-4ec6-ab94-81cac50a0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if len(state.shape)==3: #batch,rows,columns incase of parallel so len =3 else it just just rows,columns len=2\n",
    "            encoded_state = np.swapaxes(encoded_state,0,1)            \n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f607991-27be-4b89-a7e4-1151ab815a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,game,num_resBlocks,num_hidden,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3,num_hidden,kernel_size =3,padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden,32,kernel_size =3,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*game.row_count * game.column_count,game.action_size)\n",
    "        )\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden,3,kernel_size =3,padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*game.row_count * game.column_count,1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.to(device)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy,value\n",
    "        \n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden,num_hidden,kernel_size = 3,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden,num_hidden,kernel_size = 3,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x+=residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfbe7520-81c1-4233-a5ae-884d6657e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "[[[0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 1.]]]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_2.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m tensor_state = torch.tensor(encoded_state,device = device).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     16\u001b[39m model = ResNet(tictactoe, \u001b[32m4\u001b[39m, \u001b[32m64\u001b[39m, device = device)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_2.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     18\u001b[39m model.eval()\n\u001b[32m     20\u001b[39m policy, value = model(tensor_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model_2.pt'"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "print(state)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state,device = device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device = device)\n",
    "model.load_state_dict(torch.load('model_2.pt',map_location = device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value, policy)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size),policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c375c00-2b9c-4046-b3d2-826ed6ff46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None,prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        self.children = []\n",
    "        # self.expandable_moves = game.get_valid_moves(state)\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        # return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count==0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count+1))*child.prior\n",
    "    \n",
    "    def expand(self,policy):\n",
    "        for action,prob in enumerate(policy):\n",
    "            if prob>0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "                child = Node(self.game, self.args, child_state, self, action,prob)\n",
    "                self.children.append(child)\n",
    "    \n",
    "    # def simulate(self):\n",
    "    #     value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
    "    #     value = self.game.get_opponent_value(value)\n",
    "        \n",
    "    #     if is_terminal:\n",
    "    #         return value\n",
    "        \n",
    "    #     rollout_state = self.state.copy()\n",
    "    #     rollout_player = 1\n",
    "    #     while True:\n",
    "    #         valid_moves = self.game.get_valid_moves(rollout_state)\n",
    "    #         action = np.random.choice(np.where(valid_moves == 1)[0])\n",
    "    #         rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
    "    #         value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
    "    #         if is_terminal:\n",
    "    #             if rollout_player == -1:\n",
    "    #                 value = self.game.get_opponent_value(value)\n",
    "    #             return value    \n",
    "            \n",
    "    #         rollout_player = self.game.get_opponent(rollout_player)\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args,model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state,visit_count=1)\n",
    "\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state),device = self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy,axis = 1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # add noise to our model\n",
    "        policy = (1-self.args['dirichlet_epsilon'])*policy + \\\n",
    "            self.args['dirichlet_epsilon']*np.random.dirichlet([self.args['dirichlet_alpha']]*self.game.action_size)\n",
    "\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy,value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state),device = self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy,axis =1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy*=valid_moves\n",
    "                policy/=np.sum(policy)\n",
    "\n",
    "                value = value.item()\n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ab689fa-38dc-4b92-8dae-264b8a60c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self,model,optimizer,game,args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.args = args\n",
    "        self.game = game\n",
    "        self.mcts = MCTS(game,args,model)\n",
    "\n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state,player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            memory.append((neutral_state,action_probs,player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1/self.args['temperature'])\n",
    "            temperature_action_probs /=np.sum(temperature_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "            state = self.game.get_next_state(state,action,player)\n",
    "            value,is_terminal = self.game.get_value_and_terminated(state,action)\n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state,hist_action_probs,hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((self.game.get_encoded_state(hist_neutral_state),\n",
    "                                        hist_action_probs,\n",
    "                                        hist_outcome))\n",
    "                return returnMemory\n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def train(self,memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0,len(memory),self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory)-1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1,1)\n",
    "\n",
    "            state = torch.tensor(state,dtype = torch.float32, device = self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype = torch.float32, device = self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype = torch.float32, device = self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy,policy_targets)\n",
    "            value_loss = F.mse_loss(out_value,value_targets)\n",
    "            loss = policy_loss+value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory+=self.selfPlay()\n",
    "            self.model.train()\n",
    "            for epoch in range(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6527d7b5-c544-4d7e-acf3-e830e16304b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, game, args,model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states),device = self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy,axis = 1).cpu().numpy()\n",
    "        \n",
    "        # add noise to our model\n",
    "        policy = (1-self.args['dirichlet_epsilon'])*policy + \\\n",
    "            self.args['dirichlet_epsilon']*np.random.dirichlet([self.args['dirichlet_alpha']]*self.game.action_size, size=policy.shape[0])\n",
    "\n",
    "        for i,spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "    \n",
    "            spg.root = Node(self.game, self.args, states[i],visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "                \n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "                    \n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                \n",
    "                if is_terminal:\n",
    "                        node.backpropagate(value)   \n",
    "                else:\n",
    "                    spg.node = node\n",
    "            \n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "            \n",
    "            if len(expandable_spGames)>0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy,value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states),device = self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy,axis =1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "\n",
    "            for i,mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy,spg_value = policy[i],value[i]\n",
    "                \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy*=valid_moves\n",
    "                spg_policy/=np.sum(spg_policy)\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96c40688-c15a-4bbd-96fe-4822acbbe0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self,model,optimizer,game,args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.args = args\n",
    "        self.game = game\n",
    "        self.mcts = MCTSParallel(game,args,model)\n",
    "\n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for _ in range(self.args['num_parallel_games'])]\n",
    "\n",
    "        while len(spGames)>0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            \n",
    "            neutral_states = self.game.change_perspective(states,player)\n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "\n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "    \n",
    "                spg.memory.append((spg.root.state,action_probs,player))\n",
    "                \n",
    "                temperature_action_probs = action_probs ** (1/self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "                \n",
    "                spg.state = self.game.get_next_state(spg.state,action,player)\n",
    "                value,is_terminal = self.game.get_value_and_terminated(spg.state,action)\n",
    "                \n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state,hist_action_probs,hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((self.game.get_encoded_state(hist_neutral_state),\n",
    "                                            hist_action_probs,\n",
    "                                            hist_outcome))\n",
    "                    del spGames[i]\n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "        return return_memory\n",
    "            \n",
    "        \n",
    "    def train(self,memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0,len(memory),self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory)-1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1,1)\n",
    "\n",
    "            state = torch.tensor(state,dtype = torch.float32, device = self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype = torch.float32, device = self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype = torch.float32, device = self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy,policy_targets)\n",
    "            value_loss = F.mse_loss(out_value,value_targets)\n",
    "            loss = policy_loss+value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory+=self.selfPlay()\n",
    "            self.model.train()\n",
    "            for epoch in range(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "\n",
    "class SPG:\n",
    "    def __init__(self,game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e066a276-fbc1-414d-9a38-1ae3f3fcb19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e80c50977214632b99ce2cefcc25124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e551ee6d11405bab54d0ba5bf93e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0deddf71aa82498d98fdfca24c4355f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(tictactoe,4,64,device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001, weight_decay = 0.0001)\n",
    "args = {\n",
    "    'C':2,\n",
    "    'num_searches':60,\n",
    "    'num_iterations':3,\n",
    "    'num_selfPlay_iterations':500,\n",
    "    'num_parallel_games':250,\n",
    "    'num_epochs':4,\n",
    "    'batch_size':64,\n",
    "    'temperature':1.25,\n",
    "    'dirichlet_epsilon':0.25,\n",
    "    'dirichlet_alpha':0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model,optimizer,tictactoe,args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac5cdc05-415b-43a5-8ea6-db8a2b9b98f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbd129580a141b59c487d547c23bf7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce8208c76294ce6b1d683624c1a119a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58140fdadae044e3878f37139d81e861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13bd09951b741ba9b7614f7a453007b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9ddfec076347348528ee4daa0c3007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56083c957af544f486cf5725591a1a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game,9,128,device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001, weight_decay = 0.0001)\n",
    "args = {\n",
    "    'C':2,\n",
    "    'num_searches':600,\n",
    "    'num_iterations':6,\n",
    "    'num_selfPlay_iterations':500,\n",
    "    'num_parallel_games':250,\n",
    "    'num_epochs':4,\n",
    "    'batch_size':128,\n",
    "    'temperature':1.25,\n",
    "    'dirichlet_epsilon':0.25,\n",
    "    'dirichlet_alpha':0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model,optimizer,game,args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f98c10d-12be-4239-99cd-ed83dfc1a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m valid_moves = game.get_valid_moves(state)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mvalid_moves\u001b[39m\u001b[33m\"\u001b[39m, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(game.action_size) \u001b[38;5;28;01mif\u001b[39;00m valid_moves[i] == \u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m action = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid_moves[action] == \u001b[32m0\u001b[39m:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33maction not valid\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 100,\n",
    "    'dirichlet_epsilon':0.0,\n",
    "    'dirichlet_alpha':0.3\n",
    "}\n",
    "\n",
    "model = ResNet(game,9,128,device)\n",
    "model.load_state_dict(torch.load(\"model_5_ConnectFour.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args,model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a22e1b-a376-4eb6-a946-6885751224e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.6.1-cp312-cp312-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/10.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/10.6 MB 2.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/10.6 MB 2.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/10.6 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/10.6 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.2/10.6 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.6/10.6 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.2/10.6 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.6 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ccb265-9ee5-43a1-b15e-2bd461e97f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227bddd9-9e7d-4a3d-bc80-effd4895a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Layout / scaling ----------\n",
    "MARGIN = 16  # pixels margin on all sides\n",
    "MAX_SQUARE = 140  # max pixel size per square to avoid huge tiles on large monitors\n",
    "MIN_SQUARE = 60   # minimum size to keep things playable\n",
    "\n",
    "# These will be computed in `init_layout()` based on the user's display and game size\n",
    "SQUARESIZE = None\n",
    "RADIUS = None\n",
    "OFFSET_X = None\n",
    "OFFSET_Y = None\n",
    "WIDTH = None\n",
    "HEIGHT = None\n",
    "SIZE = None\n",
    "FPS = 60\n",
    "\n",
    "BLACK = (0,0,0)\n",
    "WHITE = (255,255,255)\n",
    "BLUE = (13, 71, 161)\n",
    "RED = (220, 20, 60)     # player == 1 (human)\n",
    "YELLOW = (240, 200, 0)  # player == -1 (model)\n",
    "GREEN = (50,205,50)\n",
    "\n",
    "\n",
    "def init_layout(game, scale=0.85):\n",
    "    global SQUARESIZE, RADIUS, OFFSET_X, OFFSET_Y, WIDTH, HEIGHT, SIZE\n",
    "\n",
    "    pygame.init()\n",
    "    info = pygame.display.Info()\n",
    "    screen_w, screen_h = info.current_w, info.current_h\n",
    "\n",
    "    cols = game.column_count\n",
    "    rows = game.row_count\n",
    "    # reserve space for the top preview row as well: (rows + 1)\n",
    "    available_w = max(300, screen_w - 2*MARGIN)\n",
    "    available_h = max(400, screen_h - 2*MARGIN)\n",
    "\n",
    "    # compute square limited by width and height\n",
    "    sq_w = (available_w - 2*MARGIN) // cols\n",
    "    sq_h = (available_h - 2*MARGIN) // (rows + 1)\n",
    "\n",
    "    # apply scale to make everything a bit smaller\n",
    "    computed = int(min(sq_w, sq_h) * scale)\n",
    "\n",
    "    SQUARESIZE = int(max(MIN_SQUARE, min(MAX_SQUARE, computed)))\n",
    "    RADIUS = int(SQUARESIZE/2 - 6)\n",
    "\n",
    "    # compute final window size with margins\n",
    "    WIDTH = cols * SQUARESIZE + 2*MARGIN\n",
    "    HEIGHT = (rows + 1) * SQUARESIZE + 2*MARGIN\n",
    "    OFFSET_X = MARGIN\n",
    "    OFFSET_Y = MARGIN\n",
    "    SIZE = (WIDTH, HEIGHT)\n",
    "\n",
    "    return SIZE\n",
    "\n",
    "# ---------- Drawing helpers (single final update per frame) ----------\n",
    "\n",
    "def draw_board(surface, state, game, highlight_positions=None, preview_col=None):\n",
    "    \"\"\"\n",
    "    Draws the board to `surface`. `state` uses values: 0 empty, 1 human, -1 model.\n",
    "    preview_col: optional column index to show the preview disc at the top (no update here).\n",
    "    \"\"\"\n",
    "    surface.fill(BLACK)\n",
    "\n",
    "    cols = game.column_count\n",
    "    rows = game.row_count\n",
    "\n",
    "    # draw grid and discs\n",
    "    for c in range(cols):\n",
    "        for r in range(rows):\n",
    "            x = OFFSET_X + c*SQUARESIZE\n",
    "            y = OFFSET_Y + (r+1)*SQUARESIZE\n",
    "            pygame.draw.rect(surface, BLUE, (x, y, SQUARESIZE, SQUARESIZE))\n",
    "            val = int(state[r, c])\n",
    "            center = (int(x + SQUARESIZE/2), int(y + SQUARESIZE/2))\n",
    "            if val == 0:\n",
    "                pygame.draw.circle(surface, BLACK, center, RADIUS)\n",
    "            elif val == 1:\n",
    "                pygame.draw.circle(surface, RED, center, RADIUS)\n",
    "            elif val == -1:\n",
    "                pygame.draw.circle(surface, YELLOW, center, RADIUS)\n",
    "\n",
    "    # draw preview disc at top (if any) using the preview_col\n",
    "    if preview_col is not None and 0 <= preview_col < cols:\n",
    "        px = OFFSET_X + preview_col*SQUARESIZE + SQUARESIZE//2\n",
    "        py = OFFSET_Y + SQUARESIZE//2\n",
    "        pygame.draw.circle(surface, RED, (px, py), RADIUS)\n",
    "\n",
    "    # highlight winning positions\n",
    "    if highlight_positions:\n",
    "        for (r,c) in highlight_positions:\n",
    "            cx = OFFSET_X + c*SQUARESIZE + SQUARESIZE//2\n",
    "            cy = OFFSET_Y + (r+1)*SQUARESIZE + SQUARESIZE//2\n",
    "            pygame.draw.circle(surface, GREEN, (cx, cy), RADIUS, 6)\n",
    "\n",
    "\n",
    "def animate_drop(surface, state, game, col, player_value, fps=FPS):\n",
    "    \"\"\"\n",
    "    Animate a disc falling into column `col` (visually). This routine updates\n",
    "    the display while animating and returns once the disc has visually landed.\n",
    "    It does NOT modify the logical `state` (the caller should call get_next_state).\n",
    "    \"\"\"\n",
    "    cols = game.column_count\n",
    "    rows = game.row_count\n",
    "\n",
    "    # find the bottom-most empty row\n",
    "    open_row = None\n",
    "    for r in range(rows-1, -1, -1):\n",
    "        if state[r, col] == 0:\n",
    "            open_row = r\n",
    "            break\n",
    "    if open_row is None:\n",
    "        return\n",
    "\n",
    "    x = OFFSET_X + col*SQUARESIZE + SQUARESIZE//2\n",
    "    target_y = OFFSET_Y + (open_row+1)*SQUARESIZE + SQUARESIZE//2\n",
    "    y = OFFSET_Y + SQUARESIZE//2\n",
    "\n",
    "    clock = pygame.time.Clock()\n",
    "    vel = 0.0\n",
    "    acc = 2.8\n",
    "    color = RED if player_value == 1 else YELLOW\n",
    "\n",
    "    while y < target_y:\n",
    "        clock.tick(fps)\n",
    "        vel += acc\n",
    "        y += vel\n",
    "        # draw board and falling disc\n",
    "        draw_board(surface, state, game)\n",
    "        pygame.draw.circle(surface, color, (x, min(int(y), target_y)), RADIUS)\n",
    "        pygame.display.update()\n",
    "\n",
    "    # final frame\n",
    "    draw_board(surface, state, game)\n",
    "    pygame.draw.circle(surface, color, (x, target_y), RADIUS)\n",
    "    pygame.display.update()\n",
    "\n",
    "\n",
    "# ---------- Model/MCTS integration helper ----------\n",
    "def get_model_action_from_mcts(game, mcts, state, player):\n",
    "    valid = game.get_valid_moves(state)\n",
    "    valid_indices = [i for i in range(game.action_size) if valid[i] == 1]\n",
    "    if len(valid_indices) == 0:\n",
    "        return None\n",
    "    if mcts is None:\n",
    "        return int(random.choice(valid_indices))\n",
    "    try:\n",
    "        neutral_state = game.change_perspective(state.copy(), player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = int(np.argmax(mcts_probs))\n",
    "        if valid[action] == 1:\n",
    "            return action\n",
    "        for idx in np.argsort(-np.array(mcts_probs)):\n",
    "            if valid[int(idx)] == 1:\n",
    "                return int(idx)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: mcts/model inference failed:\", e)\n",
    "    return int(random.choice(valid_indices))\n",
    "\n",
    "\n",
    "# ---------- Main GUI loop ----------\n",
    "def run_gui(game, state, mcts=None, human_value=1, model_value=-1, model_move_delay_ms=600):\n",
    "    # initialize layout based on display\n",
    "    size = init_layout(game)\n",
    "    screen = pygame.display.set_mode(size)\n",
    "    pygame.display.set_caption(\"Connect Four - GUI\")\n",
    "    font = pygame.font.SysFont(\"Arial\", max(18, SQUARESIZE//5))\n",
    "    small_font = pygame.font.SysFont(\"Arial\", max(14, SQUARESIZE//6))\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    assert state.shape == (game.row_count, game.column_count), \"State shape mismatch with game dimensions.\"\n",
    "\n",
    "    player = human_value\n",
    "    play_vs_model = True if mcts is not None else False\n",
    "    game_over = False\n",
    "    highlight = None\n",
    "    info_text = \"Your turn (Red). Click a column to drop.\"\n",
    "    last_model_move_time = 0\n",
    "    preview_col = None  # column index under mouse for preview\n",
    "\n",
    "    # initial draw\n",
    "    draw_board(screen, state, game, highlight_positions=highlight, preview_col=None)\n",
    "    pygame.display.update()\n",
    "\n",
    "    running = True\n",
    "    while running:\n",
    "        clock.tick(FPS)\n",
    "        # event handling\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                break\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_q:\n",
    "                    running = False\n",
    "                    break\n",
    "                if event.key == pygame.K_r:\n",
    "                    state[:] = game.get_initial_state()\n",
    "                    player = human_value\n",
    "                    game_over = False\n",
    "                    highlight = None\n",
    "                    info_text = \"Your turn (Red). Click a column to drop.\"\n",
    "                if event.key == pygame.K_m:\n",
    "                    play_vs_model = not play_vs_model\n",
    "                    state[:] = game.get_initial_state()\n",
    "                    player = human_value\n",
    "                    game_over = False\n",
    "                    highlight = None\n",
    "                    info_text = \"Mode toggled. \" + (\"Human vs Model.\" if play_vs_model else \"Human vs Human.\")\n",
    "\n",
    "            if event.type == pygame.MOUSEMOTION:\n",
    "                mx, my = event.pos\n",
    "                # compute column under mouse taking margins into account\n",
    "                col = (mx - OFFSET_X) // SQUARESIZE\n",
    "                if 0 <= col < game.column_count:\n",
    "                    preview_col = int(col)\n",
    "                else:\n",
    "                    preview_col = None\n",
    "\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if game_over:\n",
    "                    state[:] = game.get_initial_state()\n",
    "                    player = human_value\n",
    "                    game_over = False\n",
    "                    preview_col = None\n",
    "                    highlight = None\n",
    "                    info_text = \"Your turn (Red). Click a column to drop.\"\n",
    "                    continue\n",
    "\n",
    "                mx, my = event.pos\n",
    "                col = (mx - OFFSET_X) // SQUARESIZE\n",
    "                if col < 0 or col >= game.column_count:\n",
    "                    continue\n",
    "\n",
    "                if player == human_value:\n",
    "                    valid_moves = game.get_valid_moves(state)\n",
    "                    if valid_moves[col] == 0:\n",
    "                        info_text = \"Invalid move. Choose another column.\"\n",
    "                    else:\n",
    "                        animate_drop(screen, state, game, col, player)\n",
    "                        state = game.get_next_state(state, int(col), player)\n",
    "                        val, terminated = game.get_value_and_terminated(state, int(col))\n",
    "                        if terminated:\n",
    "                            game_over = True\n",
    "                            if val == 1:\n",
    "                                info_text = f\"{player} (You) won! Press R to restart.\"\n",
    "                            else:\n",
    "                                info_text = \"Draw! Press R to restart.\"\n",
    "                            highlight = None\n",
    "                        else:\n",
    "                            if play_vs_model:\n",
    "                                player = model_value\n",
    "                                info_text = \"Model thinking...\"\n",
    "                                last_model_move_time = pygame.time.get_ticks()\n",
    "                            else:\n",
    "                                player = model_value\n",
    "                                info_text = \"Player 2's turn.\"\n",
    "\n",
    "                else:\n",
    "                    valid_moves = game.get_valid_moves(state)\n",
    "                    if valid_moves[col] == 0:\n",
    "                        info_text = \"Invalid move. Choose another column.\"\n",
    "                    else:\n",
    "                        animate_drop(screen, state, game, col, player)\n",
    "                        state = game.get_next_state(state, int(col), player)\n",
    "                        val, terminated = game.get_value_and_terminated(state, int(col))\n",
    "                        if terminated:\n",
    "                            game_over = True\n",
    "                            if val == 1:\n",
    "                                info_text = f\"{player} (Player 2) won! Press R to restart.\"\n",
    "                            else:\n",
    "                                info_text = \"Draw! Press R to restart.\"\n",
    "                            highlight = None\n",
    "                        else:\n",
    "                            player = human_value\n",
    "                            info_text = \"Your turn (Red).\"\n",
    "\n",
    "        # model turn (non-blocking)\n",
    "        if not game_over and play_vs_model and player == model_value:\n",
    "            now = pygame.time.get_ticks()\n",
    "            if now - last_model_move_time >= model_move_delay_ms:\n",
    "                action = get_model_action_from_mcts(game, mcts, state.copy(), player)\n",
    "                if action is None:\n",
    "                    game_over = True\n",
    "                    info_text = \"No valid moves: draw.\"\n",
    "                else:\n",
    "                    animate_drop(screen, state, game, action, player)\n",
    "                    state = game.get_next_state(state, int(action), player)\n",
    "                    val, terminated = game.get_value_and_terminated(state, int(action))\n",
    "                    if terminated:\n",
    "                        game_over = True\n",
    "                        if val == 1:\n",
    "                            info_text = f\"{player} (Model) won! Press R to restart.\"\n",
    "                        else:\n",
    "                            info_text = \"Draw! Press R to restart.\"\n",
    "                        highlight = None\n",
    "                    else:\n",
    "                        player = human_value\n",
    "                        info_text = \"Your turn (Red).\"\n",
    "\n",
    "        # final draw (single update per frame)\n",
    "        draw_board(screen, state, game, highlight_positions=highlight, preview_col=preview_col if (not game_over and player==human_value) else None)\n",
    "        info_surf = font.render(info_text, True, WHITE)\n",
    "        screen.blit(info_surf, (OFFSET_X + 6, OFFSET_Y + 6))\n",
    "        mode_text = f\"Mode: {'Human vs Model' if play_vs_model else 'Human vs Human'}  |  Press M to toggle, R to restart, Q to quit\"\n",
    "        mode_surf = small_font.render(mode_text, True, WHITE)\n",
    "        screen.blit(mode_surf, (OFFSET_X + 6, HEIGHT - OFFSET_Y - small_font.get_height() - 6))\n",
    "        pygame.display.update()\n",
    "\n",
    "    pygame.quit()\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fad6ba61-4f40-4fe4-a039-6e77d36bf5f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "state = game.get_initial_state()\n",
    "player = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 100,\n",
    "    'dirichlet_epsilon':0.0,\n",
    "    'dirichlet_alpha':0.3\n",
    "}\n",
    "\n",
    "model = ResNet(game,9,128,device)\n",
    "model.load_state_dict(torch.load(\"model_5_ConnectFour.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args,model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "run_gui(game,state,mcts= mcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f59df1-af0f-4dbd-8ab5-b2eb13195cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
